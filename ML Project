---
title: 'MBA 563: Group J026 Assignment 7'
author: "Emily Lohman"
date: "7/27/2021"
output: html_document
---

*********KNN & Decision Trees*********************************

# Initial loading of data, packages, and functions
```{r}
# Run this reusable confusion matrix function (https://en.wikipedia.org/wiki/Confusion_matrix)
my_confusion_matrix <- function(cf_table) {
  true_positive <- cf_table[4]
  true_negative <- cf_table[1]
  false_positive <- cf_table[2]
  false_negative <- cf_table[3]
  accuracy <- (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)
  sensitivity_recall <- true_positive / (true_positive + false_negative) 
  specificity_selectivity <- true_negative / (true_negative + false_positive)
  precision <- true_positive / (true_positive + false_positive) 
  neg_pred_value <- true_negative/(true_negative + false_negative)
  print(cf_table)
  my_list <- list(sprintf("%1.0f = True Positive (TP), Hit", true_positive),
                  sprintf("%1.0f = True Negative (TN), Rejection", true_negative),
                  sprintf("%1.0f = False Positive (FP), Type 1 Error", false_positive),
                  sprintf("%1.0f = False Negative (FN), Type 2 Error", false_negative),
                  sprintf("%1.4f = Accuracy (TP+TN/(TP+TN+FP+FN))", accuracy), 
                  sprintf("%1.4f = Sensitivity, Recall, Hit Rate, True Positive Rate (How many positives did the model get right? TP/(TP+FN))", sensitivity_recall),
                  sprintf("%1.4f = Specificity, Selectivity, True Negative Rate (How many negatives did the model get right? TN/(TN+FP))", specificity_selectivity),
                  sprintf("%1.4f = Precision, Positive Predictive Value (How good are the model's positive predictions? TP/(TP+FP))", precision),
                  sprintf("%1.4f = Negative Predictive Value (How good are the model's negative predictions? TN/(TN+FN)", neg_pred_value)
  )
  return(my_list)
}
# Install and load packages (don't install twice)
#install.packages('tidyverse')
library(tidyverse)
# Load data
df <- read_rds("mod6HE_logit.rds")
```


*********KNN****************
# **1.0** 
# Preprocess data for knn
```{r}
# Not for the model
knn1 <- df %>% ungroup() %>% 
  select(store, week, region, high_med_rev, high_med_units, high_med_gpm)

# make the target feature a factor and put the "low" level first so `my_confusion_matrix()` works correctly
knn2 <- df %>% mutate(high_med_gp = factor(if_else(high_med_gp==1, 'high', 'low'), levels=c('low', 'high'))) 
knn2 <- knn2 %>% ungroup() %>% 
  select(high_med_gp, size, region, promo_units_per, 
         altbev_units_per, confect_units_per, salty_units_per, 
         velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)

# Data must be numeric so one-hot encode `region`
#install.packages('fastDummies') #(don't install twice)
library(fastDummies)
knn2 <- fastDummies::dummy_cols(knn2, select_columns = c("region"), remove_selected_columns=T)

# Check that "positive" is last for the `my_confusion_matrix` to work 
contrasts(knn2$high_med_gp)
```


# Partition the data
```{r}
#install.packages('caret') #(don't install twice)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=knn2$high_med_gp, p=.75, list=FALSE)
data_train <- knn2[partition, ]
data_test <- knn2[-partition, ]

# Separate the target variable from the training and testing data 
X_train <- data_train %>% select(-high_med_gp)
X_test <-  data_test %>% select(-high_med_gp) 
y_train <- data_train$high_med_gp
y_test <- data_test$high_med_gp
```


# Features must be standardized so use z-score standardization
```{r}
X_train <- scale(X_train)
X_test <- scale(X_test)
```


# Run the model
```{r}
#install.packages('class') #don't install twice
library(class)
knn_prediction = class::knn(train=X_train, test=X_test, cl=y_train, k=round(sqrt(nrow(data_train))/2))
```


# Confusion matrix - checking accuracy
```{r}
table2 <- table(knn_prediction, y_test) #prediction on left and truth on top
my_confusion_matrix(table2)
```


# Put the data back together for future use
```{r}
# Put the prediction back into the test data
data_test$knn <- knn_prediction

# Create a variable that shows if the prediction was correct
data_test <- data_test %>% 
  mutate(correct_knn = if_else(knn == high_med_gp, 'correct', 'WRONG!'))

# Add back the original data to the test data
temp1 <- knn1[-partition, ]
full_test_knn <- bind_cols(temp1, data_test)

# For viewing in class
full_test_knn <- full_test_knn %>% 
  select(store, week, high_med_gp, knn, correct_knn, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_knn, n=10)
```


**********DECISION TREES**************************************
# **2.0**
# Preprocess data
```{r}
# Not for the model
tree1 <- df %>% ungroup() %>% 
  select(store, week, high_med_rev, high_med_units, high_med_gpm)

# make the target feature and `region` a factor
tree2 <- df %>% mutate(high_med_gp = factor(if_else(high_med_gp==1, 'high', 'low'), levels=c('low', 'high')),
                       region = factor(region)) 
tree2 <- tree2 %>% ungroup() %>% 
  select(high_med_gp, size, region, promo_units_per, 
         altbev_units_per, confect_units_per, salty_units_per, 
         velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)

# Check that "positive" is last for `my_confusion_matrix()` to work 
contrasts(tree2$high_med_gp)
```


# Use the `caret` package to split the data, 75% training and 25% testing
```{r}
#install.packages('caret') #(don't install twice)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=tree2$high_med_gp, p=.75, list=FALSE)
data_train <- tree2[partition, ]
data_test <- tree2[-partition, ]
```


# Use the `rpart()` function from the `rpart` package to train the model
```{r}
#install.packages('rpart') #(don't install twice)
#install.packages('rpart.plot') #(don't install twice)
library(rpart)
library(rpart.plot)
model_tree <- rpart::rpart(high_med_gp ~ ., data_train)
```


# Use the trained model to predict whether `high_med_gp` is high or low
```{r}
predict_tree <- predict(model_tree, data_test, type='class') #`type='class'` keeps this a factor 
```


# Use the confusion matrix code above to examine the accuracy of this model
```{r}
table1 <- table(predict_tree, data_test$high_med_gp)
my_confusion_matrix(table1)
```


# Using the `plot()` function draw a labeled picture of the tree model.
```{r}
rpart.plot::rpart.plot(model_tree, box.palette = 'RdBu', shadow.col = 'gray', nn=TRUE)
```


# Put the data back together for future use
```{r}
# Put the prediction back into the test data
data_test$tree <- predict_tree

# Create a variable that shows if the prediction was correct
data_test <- data_test %>% 
  mutate(correct_tree = if_else(tree == high_med_gp, 'correct', 'WRONG!'))

# Add back the original data
temp1 <- tree1[-partition, ]
full_test_tree <- bind_cols(temp1, data_test)

# For viewing in class
full_test_tree <- full_test_tree %>% 
  select(store, week, high_med_gp, tree, correct_tree, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_tree, n=10)
```


**3.0**
# Put both predictions together
```{r}
full_test <- bind_cols(full_test_knn %>% select(store, week, high_med_gp, knn, correct_knn), 
                       full_test_tree %>% select(-store, -week, -high_med_gp))
slice_sample(full_test, n=10)
```


### 1. (0.5 points)Is type 1 or type 2 error higher for the KNN model? That is, which has a higher number?

[Type 1 error is higher for KNN. This generates more false positives.] 

### 2. Which aspect of the accuracy of the KNN model is better—sensitivity (hit rate) or specificity (true negative rate)?

#### a. (0.5 Points) Write your answer here.

[Sensitivity is better in the KNN model.]

####b. (0.5 Points) Explain the above answer. That is, which individual components of these measures (either true positive, true negative, false positive, or false negative) led to sensitivity (hit rate) or specificity (true negative rate) being better than the other?

[The sensitivity (TP/(TP+FN)) is high because there are a high number of true positives and a low number of false negatives. Specificity (TN/(TN+FP)) is not as high because there are not as many true negatives (as true positives) as well as more false positives (than false negatives).]

#### c. (0.5 Points) What does this mean about the business that this model is examining? Write less than three sentences.

[It means that this business has better visibility and predictive ability for cases where the stores are selling higher than average number of units. It will give them improved ability to analyze those positive scenarios and adopt the conclusions across the business.]


#### d. (1 Point) The nine measures of accuracy provided in the ` my_confusion_matrix()` function output are not the only measures of accuracy. Do an internet search and describe two other measures of accuracy. These measures may or may not be derived from these nine measures.


[1. Logarithmic Loss or Log Loss, works by penalising the false classifications. It works well for multi-class classification. Minimising Log Loss gives greater accuracy for the classifier.
  
2. Mean Absolute Error is the average of the difference between the Original Values and the Predicted Values. It gives us the measure of how far the predictions were from the actual output. However, they don’t gives us any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data.

3. Area Under Curve (AUC) - AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. AUC is the area under the curve of plot False Positive Rate vs True Positive Rate at different points in [0, 1].

4. F1 Score - F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).]

### 3. Interpret the decision tree output by answering the following questions:

#### a. (0.5 Points) Start at the beginning of the tree, the root node. What is the most important factor for above median units sold? That is, if you had to pick one feature that told you the most, what would it be? Said even another way, which feature keeps popping up in the tree?

[Size is the most important factor for above median units sold.]

#### b. (0.5 Points) Using the decision tree, if you are a smaller store (a store that offers less than 980 products for sale), which regions are more likely to lead to above median units sold?

[West and Quebec]


### 4.	Look back to your group assignment for Module 6 in which you were asked to use logistic regression to examine median units sold (`high_med_units`). Over the course of that assignment and this current assignment you have used three algorithms to examine this feature variable (`high_med_units`). Suppose you were working on an engagement to help NANSE better predict and understand when a store in a particular week will sell an above average number of units. In particular, suppose NANSE engaged you to answer the questions below. Respond to each question by picking which of the three algorithms is the most helpful in answering the question and explain why by comparing and contrasting to the other models. Even though your client is unlikely to care about this, for pedagogical purposes, make sure to discuss what it is about the model that you pick that makes it more helpful than the others. Your response to a. and b. should be about three sentences each.

#### a. (1 Point) “We would like to build a company-wide dashboard next year that tells us at the end of each week which stores sold enough units to be in the top half of units sold for that year, even though the year is not over. Can you use the data from the year that just ended to create a predictive model that, with a high degree of accuracy, tells us which of our stores in a given week is likely to sell above median units?”

[This classification problem will be best served by a KNN. KNN offers the highest accuracy rate when compared to other choices. Precision is also higher in the KNN model, which would improve the model's ability to predict stores selling above median units. Since the purpose is simply to identify those stores with a dashboard, but not to dig into influencing factors, KNN is a good choice. KNN is also the most intuitive algorithm with a faster training timeline. KNN does not require the exclusive use of linear data, and KNN does not fall victim to over-fitting, which can be seen with decision trees.

Alternatively, a decision tree is also a strong choice. It has somewhat similar accuracy and sensitivity levels to the KNN, and it is more efficient at handling the large datasets that are expected from this specific convienience store project. Decision trees and regressions do not require the data to be standardized prior to running the model and can learn the relationship between input and output features. However, Decision Trees are more likely to encounter over fitting when compared to KNN. 


#### b. (1 Point)“In addition to this dashboard, we would like to use last year’s data to understand which variables help our stores have successful weeks. Can you use that data to tell us which factors are most important at helping our stores have above median units sold in a given week?”

[KNN does not give information about the influencing factors, so that would be a poor choice for this use case. Two other tools will be better: 

By evaluating the decison tree, we can determine Size (the number of different items sold in a store,)  is one of the most important factors. Store which sell 980 or more items tend to be associated wth more sucessful revenue. The stores region and number of promo untits sold also contribute to the store's ability to have above median units sold in a given week.

Logistic Regression would also be a very strong choice.  Compared to decision tree, logistic regression gives more specific information about significance & coefficient magnitude for each of the factors. Although, decision tree does have better accuracy and clearer demarcation to the impact of each parameter on the final outcome.] 

### 5. Our discussions on Coursera and in class have focused on three classification algorithms (logistic regression, k-nearest neighbors, and decision trees). Many other classification algorithms exist and new ones are being developed all of the time. As you continue to expand your skills you will need to develop the ability to use the framework we are providing in this class to learn new algorithms. Do some research and find a classification algorithm that we have not discussed (not one of these three).

#### a. (0.5 Points) List this algorithm.

[Support Vector Machines(SVM’s)
A support vector machine (SVM) is a supervised binary machine learning algorithm that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text.

Mainly SVM is used for text classification problems. It classifies the unseen data. It is more widely used than Naive Bayes.

Applications:
SVMs have a number of applications in several fields like Bioinformatics, to classify genes, etc.]

#### b. (2 Points) List several advantages and disadvantages of this algorithm. If possible, compare and contrast it to the three algorithms we studied.


[Pros:
It works really well with a clear margin of separation
It is effective in high dimensional spaces.
It is effective in cases where the number of dimensions is greater than the number of samples.
It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.

Cons:
It doesn’t perform well when we have large data set because the required training time is higher
It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping
SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. It is included in the related SVC method of Python scikit-learn library.]

#### c. (0.5 Points) Find two relevant lines of R code that are used to run this algorithm and paste them below.


```{r}
#Import Library
##require(e1071) #Contains the SVM 
##Train <- read.csv(file.choose())##
##Test <- read.csv(file.choose())##
# there are various options associated with SVM training; like changing kernel, gamma and C value.

# create model
##model <- svm(Target~Predictor1+Predictor2+Predictor3,data=Train,kernel='linear',gamma=0.2,cost=100)##

#Predict Output
##preds <- predict(model,Test) table(preds)##
```

```{r}
#tune the parameters 
##sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None)##
```


